{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem context\n",
    "\n",
    "An English document has been corrupted in an unusual manner. Each alphabetic word (no numbers, but apostrophes allowed) has had every other character replaced with a `#` character. For example, a sentence like\n",
    "```\n",
    "I'm here. It's a cat.\n",
    "```\n",
    "would appear in the document as\n",
    "```\n",
    "I#m h#r#. I#'# a c#t.\n",
    "```\n",
    "\n",
    "We have extracted the tokens from this corrupted document and have provided the list of tokens to you as `corrupted_tokens.txt`. Note that the order of tokens in the list is the same as how they appeared in the original document, and that surrounding punctuation like `.` and `,` have been removed.\n",
    "\n",
    "As an example, if the uncorrupted document is:\n",
    "```\n",
    "I'm here. It's a cat.\n",
    "```\n",
    "the its corresponding corruption followed by tokenization would be:\n",
    "```\n",
    "I#m\n",
    "h#r#\n",
    "I#'#\n",
    "a\n",
    "c#t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num corrupted tokens = 29293\n",
      "first 20 corrupted tokens ->\n",
      "  T#a#\n",
      "  n#t#\n",
      "  r#c#l#s\n",
      "  a#\n",
      "  e#p#r#e#c#\n",
      "  T#e\n",
      "  p#s#e#g#r#\n",
      "  w#r#\n",
      "  s#n#\n",
      "  f#r\n",
      "  t#\n",
      "  c#m#\n",
      "  u#\n",
      "  i#\n",
      "  t#e\n",
      "  b#w\n",
      "  a#d\n",
      "  s#e\n",
      "  a\n",
      "  f#n#\n"
     ]
    }
   ],
   "source": [
    "# read in corrupted tokens\n",
    "\n",
    "CORRUPTED_TOKENS = []\n",
    "\n",
    "with open('corrupted_tokens.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tok = line.strip()\n",
    "        CORRUPTED_TOKENS.append(tok)\n",
    "\n",
    "# print a sample of the corrupted tokens\n",
    "print('num corrupted tokens =', len(CORRUPTED_TOKENS))\n",
    "print('first 20 corrupted tokens ->')\n",
    "for tok in CORRUPTED_TOKENS[:20]:\n",
    "    print(' ', tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Try to recover the corrupted tokens as best you can and write your guess of the original tokens to a file `recovered_tokens.txt`, one token per line. Each line in `recovered_tokens.txt` should match its corresponding line in `corrupted_tokens.txt`.\n",
    "\n",
    "To enable you to do this, you are given a tokenization of an uncorrupted training document, i.e. the `training_tokens.txt` file. This file also one token per line and its tokens appear in the same order as in the uncorrupted training document. You can use this along with whatever external material you deem necessary to inform your recovery of the corrupted tokens.\n",
    "\n",
    "You may use whatever language and/or tools you deem necessary.\n",
    "\n",
    "Please submit your solution consisting of:\n",
    "1. your `recovered_tokens.txt` file. This should contain the same number of tokens/lines as `corrupted_tokens.txt`. Additionally, case is not important for the recovered tokens, i.e. we treat `Prince` the same as `prince` the same as `PRINCE` .\n",
    "2. your code for your attempt in recovering the corrupted tokens\n",
    "3. a short write-up of your methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training tokens = 406223\n",
      "first 20 training tokens ->\n",
      "  PREFACE\n",
      "  Most\n",
      "  of\n",
      "  the\n",
      "  adventures\n",
      "  recorded\n",
      "  in\n",
      "  this\n",
      "  book\n",
      "  really\n",
      "  occurred\n",
      "  one\n",
      "  or\n",
      "  two\n",
      "  were\n",
      "  experiences\n",
      "  of\n",
      "  my\n",
      "  own\n",
      "  the\n"
     ]
    }
   ],
   "source": [
    "TRAINING_TOKENS = []\n",
    "with open('training_tokens.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tok = line.strip()\n",
    "        TRAINING_TOKENS.append(tok)\n",
    "\n",
    "# print a sample of the training tokens\n",
    "print('num training tokens =', len(TRAINING_TOKENS))\n",
    "print('first 20 training tokens ->')\n",
    "for tok in TRAINING_TOKENS[:20]:\n",
    "    print(' ', tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example solution\n",
    "\n",
    "As an example for how the `recovered_tokens.txt` should look like, here's a naive example solution that attempts recovery by replacing all `#` characters with `e` and generates a `recovered_tokens_EXAMPLE.txt` file with all lower-case tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example predictor that lower-cases a corrupted token and attempts recovery by replacing '#' with 'e'\n",
    "def dummy_predictor(corrupted_token):\n",
    "    return corrupted_token.lower().replace('#', 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num recovered tokens (example) = 29293\n",
      "first 20 recovered tokens (example) ->\n",
      "  teae\n",
      "  nete\n",
      "  receles\n",
      "  ae\n",
      "  eepereeece\n",
      "  tee\n",
      "  peseeegere\n",
      "  were\n",
      "  sene\n",
      "  fer\n",
      "  te\n",
      "  ceme\n",
      "  ue\n",
      "  ie\n",
      "  tee\n",
      "  bew\n",
      "  aed\n",
      "  see\n",
      "  a\n",
      "  fene\n"
     ]
    }
   ],
   "source": [
    "# recover a guess of the original tokens using the dummy predictor\n",
    "RECOVERED_TOKENS_EXAMPLE = []\n",
    "\n",
    "for tok in CORRUPTED_TOKENS:\n",
    "    recovered_tok = dummy_predictor(tok) if len(tok) > 1 else tok\n",
    "    RECOVERED_TOKENS_EXAMPLE.append(recovered_tok)\n",
    "\n",
    "# print a sample of the recovered tokens\n",
    "print('num recovered tokens (example) =', len(RECOVERED_TOKENS_EXAMPLE))\n",
    "print('first 20 recovered tokens (example) ->')\n",
    "for tok in RECOVERED_TOKENS_EXAMPLE[:20]:\n",
    "    print(' ', tok)\n",
    "    \n",
    "# write recovered tokens file from our guess of the original tokens\n",
    "with open('recovered_tokens_EXAMPLE.txt', 'w', encoding='utf-8') as f:\n",
    "    for tok in RECOVERED_TOKENS_EXAMPLE:\n",
    "        f.write('%s\\n' % tok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
